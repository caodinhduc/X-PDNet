# X-PDNet: Accurate Joint Plane Instance Segmentation and Monocular Depth Estimation with Cross-Task Attention and Boundary Correction
This is an implementation for X-PDNet: a multi-task learning framework for joint plane instance segmentation and depth estimation, which allows the respective task decoder to adaptively distill the cross-supplementary information for
the specific task optimization. Besides, we propose Depth Guided Boundary preserving Loss to precise boundary region
segmentation.
This repo is implemented by Pytorch based on [PlaneRecNet](https://github.com/EryiXie/PlaneRecNet), proposed loss and human-annotated data will be released if we get acceptance of any upcoming conference.
### 1. General architecture
![Network Architecture](/images/X-PDNet.png)
### 2. We design a cross-task attention module, which leverage the idea of PAD-Net with improvement allowing model adaptive with different scales of plane instance mask

![attention](/images/attention.png)
### 3. We focus on problem of incorrect segmentation at boundary regions, analyze the limatation of obtaining boundary from current groundtruth to design traditional boundary preserving loss

![coarse](/images/coarse.png)
### 4. Then propose Depth Guide Boundary Preserving Loss, that alleviates the problem of imperfect instance ground truth
Since my paper is under review, I cannot share more about this
### 5. Manually annotated dataset for evaluation
We contribute 3000 images with over 30000 instance masks labeled by human for reliable evaluation of plane instance segmentation task
![result1](/images/label.png)
# Result
### 1. Comparison with existing methods

![result1](/images/result1.png)
### 2. Quatitative comparison of X-PDNet with baseline
Visualization of effectiveness of Cross Attention Design on images from ScanNet dataset. For each example image, the first row is output of PlaneRecNet while the second row generated by X-PDNet, (Normal is recovered from predicted depth).

![result2](/images/result2.png)

### 3. Evaluation of Depth Guided Boundary Preserving Loss on both original and human labeled annotation

![result3](/images/result3.png)
### 4. Compare the effectiveness of proposed DGBPL to vanilla method
![result4](/images/result4.png)